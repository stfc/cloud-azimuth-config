#############################################################################
# ansible collections

### workstation
azimuth_caas_stackhpc_workstation_extra_vars_overrides:
  ansible_init_collections:
    - name: https://github.com/stfc/cloud-image-builders.git
      version: "3b0e9b3"
      type: git
  ansible_init_playbooks:
    - name: azimuth.security.run_vm_baseline

### r studio
azimuth_caas_stackhpc_rstudio_extra_vars_overrides:
  ansible_init_collections:
    - name: https://github.com/stfc/cloud-image-builders.git
      version: "3b0e9b3"
      type: git
  ansible_init_playbooks:
    - name: azimuth.security.run_vm_baseline

### repo2docker
azimuth_caas_stackhpc_repo2docker_extra_vars_overrides:
  ansible_init_collections:
    - name: https://github.com/stfc/cloud-image-builders.git
      version: "3b0e9b3"
      type: git
  ansible_init_playbooks:
    - name: azimuth.security.run_vm_baseline

#############################################################################
# Images configuration

community_images_custom_properties:
  - "hw_scsi_model=virtio-scsi"
  - "hw_disk_bus=scsi"

#############################################################################
# Configuration for the seed node (HA) or single node

# The ID of an existing network to create the node on
#infra_network_id: "5be315b7-7ebd-4254-97fe-18c1df501538"
# OR
# The CIDR of the subnet that should be created
infra_network_cidr: 192.168.100.0/24
# The ID of the external network to connect to via a router
infra_external_network_id: "5283f642-8bd8-48b6-8608-fa3006ff4539"

# The fixed floating IP to associate with the machine
# This IP must be pre-allocated to the project
# For a single node deployment, this IP should have the wildcard ingress domain assigned to it
infra_fixed_floatingip: "130.246.214.139"
# OR
# The name of the floating IP pool to allocate a floating IP from
#infra_floatingip_pool: "public"
# OR
# The ID of a provisioning network that will be used to access the seed node
# infra_provisioning_network_id: "5be315b7-7ebd-4254-97fe-18c1df501538"

# The image id of an Ubuntu 20.04 image to use for the node
#   N.B. This is populated automatically using community images by default
# infra_image_id: "bc31f48a-b79d-4cbe-8481-08717e1e643e"
# infra_image_id: ""

# The id of the flavor to use for the node
# For a seed node for an HA cluster, 8GB RAM is fine (maybe even 4GB)
# For a single node deployment, >= 16GB RAM is recommended
infra_flavor_id: "f120e108-7883-4933-bb76-4f88909447f6"  # l6.c4 (4 vCPU, ~15GB RAM, 75GB disk)

# The size in GB for the data volume
# This will hold all cluster data, including Kubernetes resources, and also PVC data
infra_data_volume_size: 100

#   If larger than zero, project specific manila share should be auto-created
# azimuth_openstack_manila_project_share_gb: 1

#############################################################################
# Configuration for the HA cluster

# The Kubernetes version that will be used for the HA cluster
#   N.B. This is populated automatically using community images by default
# capi_cluster_kubernetes_version: ""
capi_cluster_kubernetes_version: 1.29.10
# The ID of the image that will be used for the nodes of the HA clu3\ster
#   N.B. This is populated automatically using community images by default
# capi_cluster_machine_image_id: "0074f426-8fd8-45dd-820a-4927aec5fbd9"
# The name of the flavor to use for control plane nodes
capi_cluster_control_plane_flavor: "l6.c4"  # 4 vCPU, ~15GB RAM, 75GB disk
# The name of the flavor to use for worker nodes
capi_cluster_worker_flavor: "l6.c4"  # 4 vCPU, ~15GB RAM, 75GB disk
# The number of worker nodes
capi_cluster_worker_count: 3
# The fixed floating IP to associate with the load balancer for the ingress controller
# This IP must be pre-allocated to the project and should have the wildcard ingress domain assigned to it
capi_cluster_addons_ingress_load_balancer_ip: "130.246.215.76"

#### For the HA cluster ####

# A list of failure domains that should be considered for control plane nodes
# capi_cluster_control_plane_failure_domains: [ceph]
# # The failure domain for worker nodes
# capi_cluster_worker_failure_domain: ceph

# #### For tenant clusters ####

# azimuth_capi_operator_capi_helm_control_plane_failure_domains: [ceph]
# azimuth_capi_operator_capi_helm_worker_failure_domain: ceph

#-----
# Use default compute AZ for HA + tenant clusters
#-----
capi_cluster_control_plane_omit_failure_domain: true
capi_cluster_worker_failure_domain: null
azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain: true
azimuth_capi_operator_capi_helm_worker_failure_domain: null

#-----
# Volume AZ for HA + tenant clusters
#-----
capi_cluster_addons_csi_cinder_availability_zone: ceph
azimuth_capi_operator_capi_helm_csi_cinder_default_availability_zone: ceph

#-----
# Networking for tenant + HA clusters
#-----
capi_cluster_pods_cidr: 10.0.0.0/13
capi_cluster_services_cidr: 10.8.0.0/13
azimuth_capi_operator_capi_helm_pods_cidr: "{{ capi_cluster_pods_cidr }}"
azimuth_capi_operator_capi_helm_services_cidr: "{{ capi_cluster_services_cidr }}"

#-----
# Docker Hub mirror for HA + tenant clusters
#-----
capi_cluster_registry_mirrors:
  docker.io:
    - https://dockerhub.stfc.ac.uk
azimuth_capi_operator_capi_helm_registry_mirrors: "{{ capi_cluster_registry_mirrors }}"

#############################################################################
# Ingress configuration

# The base domain to use for ingress resources
ingress_base_domain: "apps.cape.stfc.ac.uk"
# Use an SSLIP domain until DNS wildcard record is sorted
# ingress_base_domain: >-
#   apps.{{ capi_cluster_addons_ingress_load_balancer_ip | replace('.', '-') }}.sslip.io

ingress_tls_enabled: true

# Indicates if cert-manager should be enabled
# Currently, TLS is enabled for ingress if cert-manager is enabled
certmanager_enabled: no

# Indicates if Harbor should be enabled to provide pull-through caches
harbor_enabled: no

#############################################################################
# Azimuth configuration

# Indicates if the Zenith app proxy should be enabled
azimuth_apps_enabled: yes
# Indicates if Kubernetes support should be enabled
azimuth_kubernetes_enabled: yes
# Indicates if Cluster-as-a-Service (CaaS) should be enabled
azimuth_clusters_enabled: yes

# The name of the current cloud
azimuth_current_cloud_name: STFC Cloud
# The label for the current cloud
azimuth_current_cloud_label: STFC Cloud

#############################################################################
# Configuration of authenticators / authentication methods

# Use the STFC domain for now
azimuth_openstack_domain: stfc

azimuth_openstack_auth_url: https://openstack.stfc.ac.uk:5000/v3

# Whether the password authenticator should be enabled (enabled by default)
azimuth_authenticator_password_enabled: true
# The label for the password authenticator
azimuth_authenticator_password_label: "Username + Password"

# Whether the appcred authenticator should be enabled (not enabled by default)
azimuth_authenticator_appcred_enabled: false
# The label for the appcred authenticator
azimuth_authenticator_appcred_label: "Application Credential"

azimuth_authenticator_federated_enabled: yes
azimuth_authenticator_federated_provider: "openid"
azimuth_authenticator_federated_protocol: "openid"

# # Whether the federated authenticator should be enabled (not enabled by default)
# azimuth_authenticator_federated_enabled: false
# # The label for the federated authenticator
# azimuth_authenticator_federated_label: "Federated"
# # The provider for the federated authenticator
# # This should correspond to the Keystone federation URL, e.g. <auth url>/auth/OS-FEDERATION/websso/<provider>
# azimuth_authenticator_federated_provider: oidc
# # azimuth_authenticator_federated_provider: https://openstack.stfc.ac.uk:5000/v3/auth/OS-FEDERATION/identity_providers/openid/protocols/openid/websso?origin=https://openstack.stfc.ac.uk/auth/websso/

#############################################################################
# Azimuth repos

# Azimuth-images
community_images_azimuth_images_repo: https://github.com/azimuth-cloud/azimuth-images
community_images_azimuth_images_version: 0.15.0

# Workstation
azimuth_caas_stackhpc_workstation_git_url: https://github.com/azimuth-cloud/caas-workstation.git
azimuth_caas_stackhpc_workstation_git_version: 0.10.0

# R-Studio
azimuth_caas_stackhpc_rstudio_git_url: https://github.com/azimuth-cloud/caas-r-studio-server
azimuth_caas_stackhpc_rstudio_git_version: 0.7.0

# Repo2docker 
azimuth_caas_stackhpc_repo2docker_git_url: https://github.com/azimuth-cloud/caas-repo2docker.git
azimuth_caas_stackhpc_repo2docker_git_version: 0.7.0

# SLURM
azimuth_caas_stackhpc_slurm_appliance_git_url: https://github.com/stackhpc/ansible-slurm-appliance.git
azimuth_caas_stackhpc_slurm_appliance_git_version: v1.155

