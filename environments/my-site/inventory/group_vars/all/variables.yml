#####
# ansible collections
#####


# azimuth_caas_stackhpc_workstation_extra_vars_overrides:
#   ansible_init_default_playbooks:
#     - name: vendor1.collection1.playbook1
#     - name: vendor1.collection1.playbook2
#       stage: pre
#     - name: vendor2.collection2.playbook1

#   ansible_init_default_collections:
#     - type: git
#       name: https://github.com/vendor1/ansible-collection1.git
#       version: 0.1.0
#     - type: galaxy
#       name: vendor2.collection2
#       version: 0.2.0  
#       source: the_fountain_of_youth



#####
# Images configuration
#####
community_images_custom_properties:
  - "hw_scsi_model=virtio-scsi"
  - "hw_disk_bus=scsi"

#####
# Configuration for the seed node (HA) or single node
#####

# The ID of an existing network to create the node on
#infra_network_id: "5be315b7-7ebd-4254-97fe-18c1df501538"
# OR
# The CIDR of the subnet that should be created
infra_network_cidr: 192.168.100.0/24
# The ID of the external network to connect to via a router
infra_external_network_id: "5283f642-8bd8-48b6-8608-fa3006ff4539"

# The fixed floating IP to associate with the machine
# This IP must be pre-allocated to the project
# For a single node deployment, this IP should have the wildcard ingress domain assigned to it
infra_fixed_floatingip: "130.246.214.139"
# OR
# The name of the floating IP pool to allocate a floating IP from
#infra_floatingip_pool: "public"
# OR
# The ID of a provisioning network that will be used to access the seed node
# infra_provisioning_network_id: "5be315b7-7ebd-4254-97fe-18c1df501538"

# The image id of an Ubuntu 20.04 image to use for the node
#   N.B. This is populated automatically using community images by default
# infra_image_id: "bc31f48a-b79d-4cbe-8481-08717e1e643e"
# infra_image_id: ""

# The id of the flavor to use for the node
# For a seed node for an HA cluster, 8GB RAM is fine (maybe even 4GB)
# For a single node deployment, >= 16GB RAM is recommended
infra_flavor_id: "f120e108-7883-4933-bb76-4f88909447f6"  # l6.c4 (4 vCPU, ~15GB RAM, 75GB disk)

# The size in GB for the data volume
# This will hold all cluster data, including Kubernetes resources, and also PVC data
infra_data_volume_size: 100

#####
# Configuration for the HA cluster
#####

# The Kubernetes version that will be used for the HA cluster
#   N.B. This is populated automatically using community images by default
# capi_cluster_kubernetes_version: ""
# capi_cluster_kubernetes_version: 1.28.7
# The ID of the image that will be used for the nodes of the HA clu3\ster
#   N.B. This is populated automatically using community images by default
# capi_cluster_machine_image_id: "<image id>"
# The name of the flavor to use for control plane nodes
capi_cluster_control_plane_flavor: "l6.c4"  # 4 vCPU, ~15GB RAM, 75GB disk
# The name of the flavor to use for worker nodes
capi_cluster_worker_flavor: "l6.c4"  # 4 vCPU, ~15GB RAM, 75GB disk
# The number of worker nodes
capi_cluster_worker_count: 3
# The fixed floating IP to associate with the load balancer for the ingress controller
# This IP must be pre-allocated to the project and should have the wildcard ingress domain assigned to it
capi_cluster_addons_ingress_load_balancer_ip: "130.246.215.76"

#### For the HA cluster ####

# A list of failure domains that should be considered for control plane nodes
# capi_cluster_control_plane_failure_domains: [ceph]
# # The failure domain for worker nodes
# capi_cluster_worker_failure_domain: ceph

# #### For tenant clusters ####

# azimuth_capi_operator_capi_helm_control_plane_failure_domains: [ceph]
# azimuth_capi_operator_capi_helm_worker_failure_domain: ceph

#-----
# Use default compute AZ for HA + tenant clusters
#-----
capi_cluster_control_plane_omit_failure_domain: true
capi_cluster_worker_failure_domain: null
azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain: true
azimuth_capi_operator_capi_helm_worker_failure_domain: null

#-----
# Volume AZ for HA + tenant clusters
#-----
capi_cluster_addons_csi_cinder_availability_zone: ceph
azimuth_capi_operator_capi_helm_csi_cinder_default_availability_zone: ceph

#-----
# Networking for tenant + HA clusters
#-----
capi_cluster_pods_cidr: 10.0.0.0/13
capi_cluster_services_cidr: 10.8.0.0/13
azimuth_capi_operator_capi_helm_pods_cidr: "{{ capi_cluster_pods_cidr }}"
azimuth_capi_operator_capi_helm_services_cidr: "{{ capi_cluster_services_cidr }}"

#-----
# Docker Hub mirror for HA + tenant clusters
#-----
capi_cluster_registry_mirrors:
  docker.io:
    - https://dockerhub.stfc.ac.uk
azimuth_capi_operator_capi_helm_registry_mirrors: "{{ capi_cluster_registry_mirrors }}"


#####
# Ingress configuration
#####
# The base domain to use for ingress resources
ingress_base_domain: "apps.cape.stfc.ac.uk"
# Use an SSLIP domain until DNS wildcard record is sorted
# ingress_base_domain: >-
#   apps.{{ capi_cluster_addons_ingress_load_balancer_ip | replace('.', '-') }}.sslip.io

# Disable TLS until a wildcard cert (or internet facing for LE) is sorted
# WARNING - DO NOT GO INTO PRODUCTION WITH THIS
ingress_tls_enabled: true

# Indicates if cert-manager should be enabled
# Currently, TLS is enabled for ingress iff cert-manager is enabled
certmanager_enabled: no

# Indicates if Harbor should be enabled to provide pull-through caches
harbor_enabled: no

#####
# Azimuth configuration
#####
# Indicates if the Zenith app proxy should be enabled
azimuth_apps_enabled: yes
# Indicates if Kubernetes support should be enabled
azimuth_kubernetes_enabled: yes
# Indicates if Cluster-as-a-Service (CaaS) should be enabled
azimuth_clusters_enabled: yes

# The name of the current cloud
azimuth_current_cloud_name: my-cloud
# The label for the current cloud
azimuth_current_cloud_label: My Cloud

# Use the STFC domain for now
azimuth_openstack_domain: stfc

#####
# Configuration of authenticators / authentication methods
#####
# Whether the password authenticator should be enabled (enabled by default)
azimuth_authenticator_password_enabled: true
# The label for the password authenticator
azimuth_authenticator_password_label: "Username + Password"

# Whether the appcred authenticator should be enabled (not enabled by default)
azimuth_authenticator_appcred_enabled: false
# The label for the appcred authenticator
azimuth_authenticator_appcred_label: "Application Credential"

# Whether the federated authenticator should be enabled (not enabled by default)
azimuth_authenticator_federated_enabled: false
# The label for the federated authenticator
azimuth_authenticator_federated_label: "Federated"
# The provider for the federated authenticator
# This should correspond to the Keystone federation URL, e.g. <auth url>/auth/OS-FEDERATION/websso/<provider>
azimuth_authenticator_federated_provider: oidc

